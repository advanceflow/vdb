root: INFO: Namespace(arch='resnet50', batch_size=64, bits='12,24,32,48', epochs=3, gamma=200, gpu='5', learning_rate=0.001, max_iter=50, num_samples=2000)
root: INFO: 12
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/ 50][Train Loss: 60.1388]
root: INFO: [Iteration:   1/ 50][Train Loss: 28.2403]
root: INFO: [Iteration:   2/ 50][Train Loss: 25.9458]
root: INFO: [Iteration:   3/ 50][Train Loss: 24.0887]
root: INFO: [Iteration:   4/ 50][Train Loss: 23.1033]
root: INFO: [Iteration:   5/ 50][Train Loss: 22.5559]
root: INFO: [Iteration:   6/ 50][Train Loss: 21.6733]
root: INFO: [Iteration:   7/ 50][Train Loss: 22.8119]
root: INFO: [Iteration:   8/ 50][Train Loss: 21.7909]
root: INFO: [Iteration:   9/ 50][Train Loss: 22.5496]
root: INFO: [Iteration:  10/ 50][Train Loss: 23.0322]
root: INFO: [Iteration:  11/ 50][Train Loss: 24.5574]
root: INFO: [Iteration:  12/ 50][Train Loss: 25.9624]
root: INFO: [Iteration:  13/ 50][Train Loss: 27.1950]
root: INFO: [Iteration:  14/ 50][Train Loss: 26.5221]
root: INFO: [Iteration:  15/ 50][Train Loss: 25.8046]
root: INFO: [Iteration:  16/ 50][Train Loss: 24.4685]
root: INFO: [Iteration:  17/ 50][Train Loss: 25.5145]
root: INFO: [Iteration:  18/ 50][Train Loss: 26.8564]
root: INFO: [Iteration:  19/ 50][Train Loss: 26.0161]
root: INFO: [Iteration:  20/ 50][Train Loss: 25.1638]
root: INFO: [Iteration:  21/ 50][Train Loss: 26.1794]
root: INFO: [Iteration:  22/ 50][Train Loss: 27.4375]
root: INFO: [Iteration:  23/ 50][Train Loss: 27.3822]
root: INFO: [Iteration:  24/ 50][Train Loss: 26.5223]
root: INFO: [Iteration:  25/ 50][Train Loss: 27.0725]
root: INFO: [Iteration:  26/ 50][Train Loss: 26.7951]
root: INFO: [Iteration:  27/ 50][Train Loss: 27.4009]
root: INFO: [Iteration:  28/ 50][Train Loss: 25.9084]
root: INFO: [Iteration:  29/ 50][Train Loss: 27.8361]
root: INFO: [Iteration:  30/ 50][Train Loss: 26.7670]
root: INFO: [Iteration:  31/ 50][Train Loss: 27.7853]
root: INFO: [Iteration:  32/ 50][Train Loss: 26.0002]
root: INFO: [Iteration:  33/ 50][Train Loss: 27.0868]
root: INFO: [Iteration:  34/ 50][Train Loss: 27.0131]
root: INFO: [Iteration:  35/ 50][Train Loss: 27.6193]
root: INFO: [Iteration:  36/ 50][Train Loss: 26.1903]
root: INFO: [Iteration:  37/ 50][Train Loss: 26.6177]
root: INFO: [Iteration:  38/ 50][Train Loss: 26.7396]
root: INFO: [Iteration:  39/ 50][Train Loss: 27.1244]
root: INFO: [Iteration:  40/ 50][Train Loss: 28.1248]
root: INFO: [Iteration:  41/ 50][Train Loss: 27.7824]
root: INFO: [Iteration:  42/ 50][Train Loss: 26.2202]
root: INFO: [Iteration:  43/ 50][Train Loss: 27.1818]
root: INFO: [Iteration:  44/ 50][Train Loss: 26.6644]
root: INFO: [Iteration:  45/ 50][Train Loss: 26.3740]
root: INFO: [Iteration:  46/ 50][Train Loss: 27.8967]
root: INFO: [Iteration:  47/ 50][Train Loss: 26.9559]
root: INFO: [Iteration:  48/ 50][Train Loss: 27.7672]
root: INFO: [Iteration:  49/ 50][Train Loss: 28.1801]
root: INFO: [Evaluation: mAP: 0.7964, top-5000 mAP: 0.8732]
root: INFO: Namespace(arch='resnet50', batch_size=64, bits='12,24,32,48', epochs=3, gamma=200, gpu='5', learning_rate=0.001, max_iter=50, num_samples=2000)
root: INFO: 24
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/ 50][Train Loss: 237.2819]
root: INFO: [Iteration:   1/ 50][Train Loss: 97.4819]
root: INFO: [Iteration:   2/ 50][Train Loss: 92.8913]
root: INFO: [Iteration:   3/ 50][Train Loss: 88.3091]
root: INFO: [Iteration:   4/ 50][Train Loss: 88.6139]
root: INFO: [Iteration:   5/ 50][Train Loss: 90.6485]
root: INFO: [Iteration:   6/ 50][Train Loss: 88.6284]
root: INFO: [Iteration:   7/ 50][Train Loss: 85.6948]
root: INFO: [Iteration:   8/ 50][Train Loss: 85.5775]
root: INFO: [Iteration:   9/ 50][Train Loss: 85.8404]
root: INFO: [Iteration:  10/ 50][Train Loss: 85.4374]
root: INFO: [Iteration:  11/ 50][Train Loss: 92.1823]
root: INFO: [Iteration:  12/ 50][Train Loss: 97.4979]
root: INFO: [Iteration:  13/ 50][Train Loss: 95.3845]
root: INFO: [Iteration:  14/ 50][Train Loss: 96.4036]
root: INFO: [Iteration:  15/ 50][Train Loss: 95.5390]
root: INFO: [Iteration:  16/ 50][Train Loss: 92.0989]
root: INFO: [Iteration:  17/ 50][Train Loss: 96.2265]
root: INFO: [Iteration:  18/ 50][Train Loss: 97.2030]
root: INFO: [Iteration:  19/ 50][Train Loss: 92.7187]
root: INFO: [Iteration:  20/ 50][Train Loss: 94.6426]
root: INFO: [Iteration:  21/ 50][Train Loss: 103.0343]
root: INFO: [Iteration:  22/ 50][Train Loss: 109.2546]
root: INFO: [Iteration:  23/ 50][Train Loss: 100.8710]
root: INFO: [Iteration:  24/ 50][Train Loss: 102.1914]
root: INFO: [Iteration:  25/ 50][Train Loss: 98.5988]
root: INFO: [Iteration:  26/ 50][Train Loss: 107.4726]
root: INFO: [Iteration:  27/ 50][Train Loss: 102.2238]
root: INFO: [Iteration:  28/ 50][Train Loss: 101.5173]
root: INFO: [Iteration:  29/ 50][Train Loss: 100.1504]
root: INFO: [Iteration:  30/ 50][Train Loss: 101.3951]
root: INFO: [Iteration:  31/ 50][Train Loss: 104.7028]
root: INFO: [Iteration:  32/ 50][Train Loss: 102.7478]
root: INFO: [Iteration:  33/ 50][Train Loss: 107.9955]
root: INFO: [Iteration:  34/ 50][Train Loss: 107.2759]
root: INFO: [Iteration:  35/ 50][Train Loss: 105.6484]
root: INFO: [Iteration:  36/ 50][Train Loss: 100.7375]
root: INFO: [Iteration:  37/ 50][Train Loss: 110.3522]
root: INFO: [Iteration:  38/ 50][Train Loss: 102.2583]
root: INFO: [Iteration:  39/ 50][Train Loss: 102.6262]
root: INFO: [Iteration:  40/ 50][Train Loss: 106.0068]
root: INFO: [Iteration:  41/ 50][Train Loss: 105.7674]
root: INFO: [Iteration:  42/ 50][Train Loss: 107.6324]
root: INFO: [Iteration:  43/ 50][Train Loss: 102.4194]
root: INFO: [Iteration:  44/ 50][Train Loss: 109.3246]
root: INFO: [Iteration:  45/ 50][Train Loss: 107.0371]
root: INFO: [Iteration:  46/ 50][Train Loss: 107.0140]
root: INFO: [Iteration:  47/ 50][Train Loss: 104.5531]
root: INFO: [Iteration:  48/ 50][Train Loss: 104.2151]
root: INFO: [Iteration:  49/ 50][Train Loss: 107.2601]
root: INFO: [Evaluation: mAP: 0.8304, top-5000 mAP: 0.9049]
root: INFO: Namespace(arch='resnet50', batch_size=64, bits='12,24,32,48', epochs=3, gamma=200, gpu='5', learning_rate=0.001, max_iter=50, num_samples=2000)
root: INFO: 32
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/ 50][Train Loss: 416.8096]
root: INFO: [Iteration:   1/ 50][Train Loss: 175.5988]
root: INFO: [Iteration:   2/ 50][Train Loss: 160.1929]
root: INFO: [Iteration:   3/ 50][Train Loss: 162.8374]
root: INFO: [Iteration:   4/ 50][Train Loss: 168.5824]
root: INFO: [Iteration:   5/ 50][Train Loss: 162.9345]
root: INFO: [Iteration:   6/ 50][Train Loss: 166.3766]
root: INFO: [Iteration:   7/ 50][Train Loss: 165.1295]
root: INFO: [Iteration:   8/ 50][Train Loss: 150.7288]
root: INFO: [Iteration:   9/ 50][Train Loss: 158.5511]
root: INFO: [Iteration:  10/ 50][Train Loss: 160.9843]
root: INFO: [Iteration:  11/ 50][Train Loss: 174.1361]
root: INFO: [Iteration:  12/ 50][Train Loss: 172.7750]
root: INFO: [Iteration:  13/ 50][Train Loss: 173.4389]
root: INFO: [Iteration:  14/ 50][Train Loss: 166.3616]
root: INFO: [Iteration:  15/ 50][Train Loss: 166.8703]
root: INFO: [Iteration:  16/ 50][Train Loss: 173.1264]
root: INFO: [Iteration:  17/ 50][Train Loss: 160.3926]
root: INFO: [Iteration:  18/ 50][Train Loss: 161.0768]
root: INFO: [Iteration:  19/ 50][Train Loss: 170.4384]
root: INFO: [Iteration:  20/ 50][Train Loss: 161.0909]
root: INFO: [Iteration:  21/ 50][Train Loss: 183.2170]
root: INFO: [Iteration:  22/ 50][Train Loss: 186.3923]
root: INFO: [Iteration:  23/ 50][Train Loss: 179.7761]
root: INFO: [Iteration:  24/ 50][Train Loss: 179.8086]
root: INFO: [Iteration:  25/ 50][Train Loss: 187.3598]
root: INFO: [Iteration:  26/ 50][Train Loss: 183.6180]
root: INFO: [Iteration:  27/ 50][Train Loss: 187.0258]
root: INFO: [Iteration:  28/ 50][Train Loss: 185.3025]
root: INFO: [Iteration:  29/ 50][Train Loss: 179.9334]
root: INFO: [Iteration:  30/ 50][Train Loss: 182.0033]
root: INFO: [Iteration:  31/ 50][Train Loss: 186.6958]
root: INFO: [Iteration:  32/ 50][Train Loss: 185.3568]
root: INFO: [Iteration:  33/ 50][Train Loss: 186.7637]
root: INFO: [Iteration:  34/ 50][Train Loss: 187.8325]
root: INFO: [Iteration:  35/ 50][Train Loss: 184.7758]
root: INFO: [Iteration:  36/ 50][Train Loss: 188.2086]
root: INFO: [Iteration:  37/ 50][Train Loss: 181.0982]
root: INFO: [Iteration:  38/ 50][Train Loss: 193.4727]
root: INFO: [Iteration:  39/ 50][Train Loss: 187.9018]
root: INFO: [Iteration:  40/ 50][Train Loss: 182.9130]
root: INFO: [Iteration:  41/ 50][Train Loss: 194.3363]
root: INFO: [Iteration:  42/ 50][Train Loss: 183.2618]
root: INFO: [Iteration:  43/ 50][Train Loss: 187.3405]
root: INFO: [Iteration:  44/ 50][Train Loss: 193.6839]
root: INFO: [Iteration:  45/ 50][Train Loss: 190.2930]
root: INFO: [Iteration:  46/ 50][Train Loss: 191.9779]
root: INFO: [Iteration:  47/ 50][Train Loss: 193.0388]
root: INFO: [Iteration:  48/ 50][Train Loss: 186.6503]
root: INFO: [Iteration:  49/ 50][Train Loss: 192.4517]
root: INFO: [Evaluation: mAP: 0.8314, top-5000 mAP: 0.9088]
root: INFO: Namespace(arch='resnet50', batch_size=64, bits='12,24,32,48', epochs=3, gamma=200, gpu='5', learning_rate=0.001, max_iter=50, num_samples=2000)
root: INFO: 48
root: INFO: [Comment: learning rate decay]
root: INFO: [Iteration:   0/ 50][Train Loss: 917.2292]
root: INFO: [Iteration:   1/ 50][Train Loss: 463.4043]
root: INFO: [Iteration:   2/ 50][Train Loss: 440.6612]
root: INFO: [Iteration:   3/ 50][Train Loss: 432.7829]
root: INFO: [Iteration:   4/ 50][Train Loss: 439.5651]
root: INFO: [Iteration:   5/ 50][Train Loss: 415.6777]
root: INFO: [Iteration:   6/ 50][Train Loss: 375.7090]
root: INFO: [Iteration:   7/ 50][Train Loss: 391.0408]
root: INFO: [Iteration:   8/ 50][Train Loss: 403.2226]
root: INFO: [Iteration:   9/ 50][Train Loss: 394.9055]
root: INFO: [Iteration:  10/ 50][Train Loss: 375.0032]
root: INFO: [Iteration:  11/ 50][Train Loss: 407.1789]
root: INFO: [Iteration:  12/ 50][Train Loss: 402.0179]
root: INFO: [Iteration:  13/ 50][Train Loss: 415.8355]
root: INFO: [Iteration:  14/ 50][Train Loss: 407.3415]
root: INFO: [Iteration:  15/ 50][Train Loss: 402.9994]
root: INFO: [Iteration:  16/ 50][Train Loss: 380.2367]
root: INFO: [Iteration:  17/ 50][Train Loss: 397.4095]
root: INFO: [Iteration:  18/ 50][Train Loss: 389.2638]
root: INFO: [Iteration:  19/ 50][Train Loss: 389.8356]
root: INFO: [Iteration:  20/ 50][Train Loss: 388.2222]
root: INFO: [Iteration:  21/ 50][Train Loss: 422.6541]
root: INFO: [Iteration:  22/ 50][Train Loss: 438.4636]
root: INFO: [Iteration:  23/ 50][Train Loss: 430.6414]
root: INFO: [Iteration:  24/ 50][Train Loss: 412.2647]
root: INFO: [Iteration:  25/ 50][Train Loss: 430.8418]
root: INFO: [Iteration:  26/ 50][Train Loss: 417.4463]
root: INFO: [Iteration:  27/ 50][Train Loss: 431.5585]
root: INFO: [Iteration:  28/ 50][Train Loss: 418.6792]
root: INFO: [Iteration:  29/ 50][Train Loss: 455.2439]
root: INFO: [Iteration:  30/ 50][Train Loss: 431.0321]
root: INFO: [Iteration:  31/ 50][Train Loss: 442.4786]
root: INFO: [Iteration:  32/ 50][Train Loss: 431.0602]
root: INFO: [Iteration:  33/ 50][Train Loss: 447.7635]
root: INFO: [Iteration:  34/ 50][Train Loss: 438.3791]
root: INFO: [Iteration:  35/ 50][Train Loss: 429.3819]
root: INFO: [Iteration:  36/ 50][Train Loss: 431.6857]
root: INFO: [Iteration:  37/ 50][Train Loss: 430.1071]
root: INFO: [Iteration:  38/ 50][Train Loss: 429.0394]
root: INFO: [Iteration:  39/ 50][Train Loss: 432.8266]
root: INFO: [Iteration:  40/ 50][Train Loss: 433.0134]
root: INFO: [Iteration:  41/ 50][Train Loss: 434.0221]
root: INFO: [Iteration:  42/ 50][Train Loss: 446.4714]
root: INFO: [Iteration:  43/ 50][Train Loss: 444.0436]
root: INFO: [Iteration:  44/ 50][Train Loss: 435.2693]
root: INFO: [Iteration:  45/ 50][Train Loss: 434.0941]
root: INFO: [Iteration:  46/ 50][Train Loss: 420.0852]
root: INFO: [Iteration:  47/ 50][Train Loss: 425.1858]
root: INFO: [Iteration:  48/ 50][Train Loss: 451.6852]
root: INFO: [Iteration:  49/ 50][Train Loss: 453.1767]
root: INFO: [Evaluation: mAP: 0.8376, top-5000 mAP: 0.9119]
